
############################################################
# Long Sequence Benchmark - Order 5
# CL Method: ella
# Run Number: 2
# Seed: 73
# Output Base: logs_and_outputs/ella/long/order_5/outputs2
# Log File: logs_and_outputs/ella/long/order_5/logs/train_and_infer2.log
# Tasks: MultiRC -> BoolQA -> WiC -> MNLI -> CB -> COPA -> QQP -> RTE -> IMDB -> SST-2 -> dbpedia -> agnews -> yelp -> amazon -> yahoo
############################################################


============================================================
Task 1: MultiRC
============================================================
Command: deepspeed --master_port 29267 src/run_uie_lora.py --do_train --do_predict --predict_with_generate --data_dir CL_Benchmark --instruction_file configs/instruction_config.json --instruction_strategy single --per_device_train_batch_size 8 --per_device_eval_batch_size 128 --gradient_accumulation_steps 4 --learning_rate 1e-03 --deepspeed configs/ds_configs/stage2.config --max_source_length 512 --max_target_length 50 --generation_max_length 50 --add_task_name True --add_dataset_name True --overwrite_output_dir --overwrite_cache --lr_scheduler_type constant --warmup_steps 0 --logging_strategy steps --logging_steps 10 --eval_strategy no --save_strategy no --save_steps 1500 --cl_method ella --seed 73 --max_num_instances_per_task 1000 --model_name_or_path initial_model/t5-large --task_config_dir configs/long/order5_configs/MultiRC --output_dir logs_and_outputs/ella/long/order_5/outputs2/1-MultiRC --run_name long_round1 --num_train_epochs 1 --lamda_1 0 --lamda_2 0
Log file: logs_and_outputs/ella/long/order_5/logs/train_and_infer2.log

✓ Task 1 (MultiRC) completed successfully
Sleeping for 5 seconds before next task...

============================================================
Task 2: BoolQA
============================================================
Command: deepspeed --master_port 29267 src/run_uie_lora.py --do_train --do_predict --predict_with_generate --data_dir CL_Benchmark --instruction_file configs/instruction_config.json --instruction_strategy single --per_device_train_batch_size 8 --per_device_eval_batch_size 128 --gradient_accumulation_steps 4 --learning_rate 1e-03 --deepspeed configs/ds_configs/stage2.config --max_source_length 512 --max_target_length 50 --generation_max_length 50 --add_task_name True --add_dataset_name True --overwrite_output_dir --overwrite_cache --lr_scheduler_type constant --warmup_steps 0 --logging_strategy steps --logging_steps 10 --eval_strategy no --save_strategy no --save_steps 1500 --cl_method ella --seed 73 --max_num_instances_per_task 1000 --model_name_or_path logs_and_outputs/ella/long/order_5/outputs2/1-MultiRC/adapter --task_config_dir configs/long/order5_configs/BoolQA --output_dir logs_and_outputs/ella/long/order_5/outputs2/2-BoolQA --run_name long_round2 --num_train_epochs 1 --lamda_1 5000000.0 --lamda_2 0
Log file: logs_and_outputs/ella/long/order_5/logs/train_and_infer2.log

✓ Task 2 (BoolQA) completed successfully
Sleeping for 5 seconds before next task...

============================================================
Task 3: WiC
============================================================
Command: deepspeed --master_port 29267 src/run_uie_lora.py --do_train --do_predict --predict_with_generate --data_dir CL_Benchmark --instruction_file configs/instruction_config.json --instruction_strategy single --per_device_train_batch_size 8 --per_device_eval_batch_size 128 --gradient_accumulation_steps 4 --learning_rate 1e-03 --deepspeed configs/ds_configs/stage2.config --max_source_length 512 --max_target_length 50 --generation_max_length 50 --add_task_name True --add_dataset_name True --overwrite_output_dir --overwrite_cache --lr_scheduler_type constant --warmup_steps 0 --logging_strategy steps --logging_steps 10 --eval_strategy no --save_strategy no --save_steps 1500 --cl_method ella --seed 73 --max_num_instances_per_task 1000 --model_name_or_path logs_and_outputs/ella/long/order_5/outputs2/2-BoolQA/adapter --task_config_dir configs/long/order5_configs/WiC --output_dir logs_and_outputs/ella/long/order_5/outputs2/3-WiC --run_name long_round3 --num_train_epochs 1 --lamda_1 5000000.0 --lamda_2 0
Log file: logs_and_outputs/ella/long/order_5/logs/train_and_infer2.log

✓ Task 3 (WiC) completed successfully
Sleeping for 5 seconds before next task...

============================================================
Task 4: MNLI
============================================================
Command: deepspeed --master_port 29267 src/run_uie_lora.py --do_train --do_predict --predict_with_generate --data_dir CL_Benchmark --instruction_file configs/instruction_config.json --instruction_strategy single --per_device_train_batch_size 8 --per_device_eval_batch_size 128 --gradient_accumulation_steps 4 --learning_rate 1e-03 --deepspeed configs/ds_configs/stage2.config --max_source_length 512 --max_target_length 50 --generation_max_length 50 --add_task_name True --add_dataset_name True --overwrite_output_dir --overwrite_cache --lr_scheduler_type constant --warmup_steps 0 --logging_strategy steps --logging_steps 10 --eval_strategy no --save_strategy no --save_steps 1500 --cl_method ella --seed 73 --max_num_instances_per_task 1000 --model_name_or_path logs_and_outputs/ella/long/order_5/outputs2/3-WiC/adapter --task_config_dir configs/long/order5_configs/MNLI --output_dir logs_and_outputs/ella/long/order_5/outputs2/4-MNLI --run_name long_round4 --num_train_epochs 1 --lamda_1 5000000.0 --lamda_2 0
Log file: logs_and_outputs/ella/long/order_5/logs/train_and_infer2.log

✓ Task 4 (MNLI) completed successfully
Sleeping for 5 seconds before next task...

============================================================
Task 5: CB
============================================================
Command: deepspeed --master_port 29267 src/run_uie_lora.py --do_train --do_predict --predict_with_generate --data_dir CL_Benchmark --instruction_file configs/instruction_config.json --instruction_strategy single --per_device_train_batch_size 8 --per_device_eval_batch_size 128 --gradient_accumulation_steps 4 --learning_rate 1e-03 --deepspeed configs/ds_configs/stage2.config --max_source_length 512 --max_target_length 50 --generation_max_length 50 --add_task_name True --add_dataset_name True --overwrite_output_dir --overwrite_cache --lr_scheduler_type constant --warmup_steps 0 --logging_strategy steps --logging_steps 10 --eval_strategy no --save_strategy no --save_steps 1500 --cl_method ella --seed 73 --max_num_instances_per_task 1000 --model_name_or_path logs_and_outputs/ella/long/order_5/outputs2/4-MNLI/adapter --task_config_dir configs/long/order5_configs/CB --output_dir logs_and_outputs/ella/long/order_5/outputs2/5-CB --run_name long_round5 --num_train_epochs 1 --lamda_1 5000000.0 --lamda_2 0
Log file: logs_and_outputs/ella/long/order_5/logs/train_and_infer2.log

✓ Task 5 (CB) completed successfully
Sleeping for 5 seconds before next task...

============================================================
Task 6: COPA
============================================================
Command: deepspeed --master_port 29267 src/run_uie_lora.py --do_train --do_predict --predict_with_generate --data_dir CL_Benchmark --instruction_file configs/instruction_config.json --instruction_strategy single --per_device_train_batch_size 8 --per_device_eval_batch_size 128 --gradient_accumulation_steps 4 --learning_rate 1e-03 --deepspeed configs/ds_configs/stage2.config --max_source_length 512 --max_target_length 50 --generation_max_length 50 --add_task_name True --add_dataset_name True --overwrite_output_dir --overwrite_cache --lr_scheduler_type constant --warmup_steps 0 --logging_strategy steps --logging_steps 10 --eval_strategy no --save_strategy no --save_steps 1500 --cl_method ella --seed 73 --max_num_instances_per_task 1000 --model_name_or_path logs_and_outputs/ella/long/order_5/outputs2/5-CB/adapter --task_config_dir configs/long/order5_configs/COPA --output_dir logs_and_outputs/ella/long/order_5/outputs2/6-COPA --run_name long_round6 --num_train_epochs 1 --lamda_1 5000000.0 --lamda_2 0
Log file: logs_and_outputs/ella/long/order_5/logs/train_and_infer2.log

✓ Task 6 (COPA) completed successfully
Sleeping for 5 seconds before next task...

============================================================
Task 7: QQP
============================================================
Command: deepspeed --master_port 29267 src/run_uie_lora.py --do_train --do_predict --predict_with_generate --data_dir CL_Benchmark --instruction_file configs/instruction_config.json --instruction_strategy single --per_device_train_batch_size 8 --per_device_eval_batch_size 128 --gradient_accumulation_steps 4 --learning_rate 1e-03 --deepspeed configs/ds_configs/stage2.config --max_source_length 512 --max_target_length 50 --generation_max_length 50 --add_task_name True --add_dataset_name True --overwrite_output_dir --overwrite_cache --lr_scheduler_type constant --warmup_steps 0 --logging_strategy steps --logging_steps 10 --eval_strategy no --save_strategy no --save_steps 1500 --cl_method ella --seed 73 --max_num_instances_per_task 1000 --model_name_or_path logs_and_outputs/ella/long/order_5/outputs2/6-COPA/adapter --task_config_dir configs/long/order5_configs/QQP --output_dir logs_and_outputs/ella/long/order_5/outputs2/7-QQP --run_name long_round7 --num_train_epochs 1 --lamda_1 5000000.0 --lamda_2 0
Log file: logs_and_outputs/ella/long/order_5/logs/train_and_infer2.log

✓ Task 7 (QQP) completed successfully
Sleeping for 5 seconds before next task...

============================================================
Task 8: RTE
============================================================
Command: deepspeed --master_port 29267 src/run_uie_lora.py --do_train --do_predict --predict_with_generate --data_dir CL_Benchmark --instruction_file configs/instruction_config.json --instruction_strategy single --per_device_train_batch_size 8 --per_device_eval_batch_size 128 --gradient_accumulation_steps 4 --learning_rate 1e-03 --deepspeed configs/ds_configs/stage2.config --max_source_length 512 --max_target_length 50 --generation_max_length 50 --add_task_name True --add_dataset_name True --overwrite_output_dir --overwrite_cache --lr_scheduler_type constant --warmup_steps 0 --logging_strategy steps --logging_steps 10 --eval_strategy no --save_strategy no --save_steps 1500 --cl_method ella --seed 73 --max_num_instances_per_task 1000 --model_name_or_path logs_and_outputs/ella/long/order_5/outputs2/7-QQP/adapter --task_config_dir configs/long/order5_configs/RTE --output_dir logs_and_outputs/ella/long/order_5/outputs2/8-RTE --run_name long_round8 --num_train_epochs 1 --lamda_1 5000000.0 --lamda_2 0
Log file: logs_and_outputs/ella/long/order_5/logs/train_and_infer2.log

✓ Task 8 (RTE) completed successfully
Sleeping for 5 seconds before next task...

============================================================
Task 9: IMDB
============================================================
Command: deepspeed --master_port 29267 src/run_uie_lora.py --do_train --do_predict --predict_with_generate --data_dir CL_Benchmark --instruction_file configs/instruction_config.json --instruction_strategy single --per_device_train_batch_size 8 --per_device_eval_batch_size 128 --gradient_accumulation_steps 4 --learning_rate 1e-03 --deepspeed configs/ds_configs/stage2.config --max_source_length 512 --max_target_length 50 --generation_max_length 50 --add_task_name True --add_dataset_name True --overwrite_output_dir --overwrite_cache --lr_scheduler_type constant --warmup_steps 0 --logging_strategy steps --logging_steps 10 --eval_strategy no --save_strategy no --save_steps 1500 --cl_method ella --seed 73 --max_num_instances_per_task 1000 --model_name_or_path logs_and_outputs/ella/long/order_5/outputs2/8-RTE/adapter --task_config_dir configs/long/order5_configs/IMDB --output_dir logs_and_outputs/ella/long/order_5/outputs2/9-IMDB --run_name long_round9 --num_train_epochs 1 --lamda_1 5000000.0 --lamda_2 0
Log file: logs_and_outputs/ella/long/order_5/logs/train_and_infer2.log

✓ Task 9 (IMDB) completed successfully
Sleeping for 5 seconds before next task...

============================================================
Task 10: SST-2
============================================================
Command: deepspeed --master_port 29267 src/run_uie_lora.py --do_train --do_predict --predict_with_generate --data_dir CL_Benchmark --instruction_file configs/instruction_config.json --instruction_strategy single --per_device_train_batch_size 8 --per_device_eval_batch_size 128 --gradient_accumulation_steps 4 --learning_rate 1e-03 --deepspeed configs/ds_configs/stage2.config --max_source_length 512 --max_target_length 50 --generation_max_length 50 --add_task_name True --add_dataset_name True --overwrite_output_dir --overwrite_cache --lr_scheduler_type constant --warmup_steps 0 --logging_strategy steps --logging_steps 10 --eval_strategy no --save_strategy no --save_steps 1500 --cl_method ella --seed 73 --max_num_instances_per_task 1000 --model_name_or_path logs_and_outputs/ella/long/order_5/outputs2/9-IMDB/adapter --task_config_dir configs/long/order5_configs/SST-2 --output_dir logs_and_outputs/ella/long/order_5/outputs2/10-SST-2 --run_name long_round10 --num_train_epochs 1 --lamda_1 5000000.0 --lamda_2 0
Log file: logs_and_outputs/ella/long/order_5/logs/train_and_infer2.log

✓ Task 10 (SST-2) completed successfully
Sleeping for 5 seconds before next task...

============================================================
Task 11: dbpedia
============================================================
Command: deepspeed --master_port 29267 src/run_uie_lora.py --do_train --do_predict --predict_with_generate --data_dir CL_Benchmark --instruction_file configs/instruction_config.json --instruction_strategy single --per_device_train_batch_size 8 --per_device_eval_batch_size 128 --gradient_accumulation_steps 4 --learning_rate 1e-03 --deepspeed configs/ds_configs/stage2.config --max_source_length 512 --max_target_length 50 --generation_max_length 50 --add_task_name True --add_dataset_name True --overwrite_output_dir --overwrite_cache --lr_scheduler_type constant --warmup_steps 0 --logging_strategy steps --logging_steps 10 --eval_strategy no --save_strategy no --save_steps 1500 --cl_method ella --seed 73 --max_num_instances_per_task 1000 --model_name_or_path logs_and_outputs/ella/long/order_5/outputs2/10-SST-2/adapter --task_config_dir configs/long/order5_configs/dbpedia --output_dir logs_and_outputs/ella/long/order_5/outputs2/11-dbpedia --run_name long_round11 --num_train_epochs 1 --lamda_1 5000000.0 --lamda_2 0
Log file: logs_and_outputs/ella/long/order_5/logs/train_and_infer2.log

✓ Task 11 (dbpedia) completed successfully
Sleeping for 5 seconds before next task...

============================================================
Task 12: agnews
============================================================
Command: deepspeed --master_port 29267 src/run_uie_lora.py --do_train --do_predict --predict_with_generate --data_dir CL_Benchmark --instruction_file configs/instruction_config.json --instruction_strategy single --per_device_train_batch_size 8 --per_device_eval_batch_size 128 --gradient_accumulation_steps 4 --learning_rate 1e-03 --deepspeed configs/ds_configs/stage2.config --max_source_length 512 --max_target_length 50 --generation_max_length 50 --add_task_name True --add_dataset_name True --overwrite_output_dir --overwrite_cache --lr_scheduler_type constant --warmup_steps 0 --logging_strategy steps --logging_steps 10 --eval_strategy no --save_strategy no --save_steps 1500 --cl_method ella --seed 73 --max_num_instances_per_task 1000 --model_name_or_path logs_and_outputs/ella/long/order_5/outputs2/11-dbpedia/adapter --task_config_dir configs/long/order5_configs/agnews --output_dir logs_and_outputs/ella/long/order_5/outputs2/12-agnews --run_name long_round12 --num_train_epochs 1 --lamda_1 5000000.0 --lamda_2 0
Log file: logs_and_outputs/ella/long/order_5/logs/train_and_infer2.log

✓ Task 12 (agnews) completed successfully
Sleeping for 5 seconds before next task...

============================================================
Task 13: yelp
============================================================
Command: deepspeed --master_port 29267 src/run_uie_lora.py --do_train --do_predict --predict_with_generate --data_dir CL_Benchmark --instruction_file configs/instruction_config.json --instruction_strategy single --per_device_train_batch_size 8 --per_device_eval_batch_size 128 --gradient_accumulation_steps 4 --learning_rate 1e-03 --deepspeed configs/ds_configs/stage2.config --max_source_length 512 --max_target_length 50 --generation_max_length 50 --add_task_name True --add_dataset_name True --overwrite_output_dir --overwrite_cache --lr_scheduler_type constant --warmup_steps 0 --logging_strategy steps --logging_steps 10 --eval_strategy no --save_strategy no --save_steps 1500 --cl_method ella --seed 73 --max_num_instances_per_task 1000 --model_name_or_path logs_and_outputs/ella/long/order_5/outputs2/12-agnews/adapter --task_config_dir configs/long/order5_configs/yelp --output_dir logs_and_outputs/ella/long/order_5/outputs2/13-yelp --run_name long_round13 --num_train_epochs 1 --lamda_1 50000000.0 --lamda_2 0
Log file: logs_and_outputs/ella/long/order_5/logs/train_and_infer2.log

✓ Task 13 (yelp) completed successfully
Sleeping for 5 seconds before next task...

============================================================
Task 14: amazon
============================================================
Command: deepspeed --master_port 29267 src/run_uie_lora.py --do_train --do_predict --predict_with_generate --data_dir CL_Benchmark --instruction_file configs/instruction_config.json --instruction_strategy single --per_device_train_batch_size 8 --per_device_eval_batch_size 128 --gradient_accumulation_steps 4 --learning_rate 1e-03 --deepspeed configs/ds_configs/stage2.config --max_source_length 512 --max_target_length 50 --generation_max_length 50 --add_task_name True --add_dataset_name True --overwrite_output_dir --overwrite_cache --lr_scheduler_type constant --warmup_steps 0 --logging_strategy steps --logging_steps 10 --eval_strategy no --save_strategy no --save_steps 1500 --cl_method ella --seed 73 --max_num_instances_per_task 1000 --model_name_or_path logs_and_outputs/ella/long/order_5/outputs2/13-yelp/adapter --task_config_dir configs/long/order5_configs/amazon --output_dir logs_and_outputs/ella/long/order_5/outputs2/14-amazon --run_name long_round14 --num_train_epochs 1 --lamda_1 50000000.0 --lamda_2 0
Log file: logs_and_outputs/ella/long/order_5/logs/train_and_infer2.log

✓ Task 14 (amazon) completed successfully
Sleeping for 5 seconds before next task...

============================================================
Task 15: yahoo
============================================================
Command: deepspeed --master_port 29267 src/run_uie_lora.py --do_train --do_predict --predict_with_generate --data_dir CL_Benchmark --instruction_file configs/instruction_config.json --instruction_strategy single --per_device_train_batch_size 8 --per_device_eval_batch_size 128 --gradient_accumulation_steps 4 --learning_rate 1e-03 --deepspeed configs/ds_configs/stage2.config --max_source_length 512 --max_target_length 50 --generation_max_length 50 --add_task_name True --add_dataset_name True --overwrite_output_dir --overwrite_cache --lr_scheduler_type constant --warmup_steps 0 --logging_strategy steps --logging_steps 10 --eval_strategy no --save_strategy no --save_steps 1500 --cl_method ella --seed 73 --max_num_instances_per_task 1000 --model_name_or_path logs_and_outputs/ella/long/order_5/outputs2/14-amazon/adapter --task_config_dir configs/long/order5_configs/yahoo --output_dir logs_and_outputs/ella/long/order_5/outputs2/15-yahoo --run_name long_round15 --num_train_epochs 1 --lamda_1 50000000.0 --lamda_2 0
Log file: logs_and_outputs/ella/long/order_5/logs/train_and_infer2.log

✓ Task 15 (yahoo) completed successfully

============================================================
All tasks completed successfully!
Full log: logs_and_outputs/ella/long/order_5/logs/train_and_infer2.log
============================================================

